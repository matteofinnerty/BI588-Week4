---
title: "BI588-Week4-Module8"
output: html_document
date: "2025-02-11"
---

**Week 4: Module 8 Notes*

Announcements
- Homework 3 due March 3rd. 
- Choose replication paper by 2/18

**Statistical Inference**
Trying to draw conclusion about a population based on measurements from a noisy sample or trying to evaluate whether it is reasonable to assume that our sample is drawn from a particular population. 

Complications: 
a. our sample may be biased
b. there may be confounding variables
c. 

**Probability**
Summarize the relative frequencies of possible outcomes. 
Applied to the population level by describing the magnitude of chance associated with particular observations. 
Probabilities vary between 0 and 1. 

*We will use the {manipulate} package. To run manipulate effectively you must run the code directly in the console, it does not work in R markdown documents. Once run, the graph will appear in the Plots tab in the bottom right. 

Importing {manipulate}
```{r}
library(manipulate)


```

Rolling a die example
```{r}
#RUN THIS IN THE CONSOLE
##outcomes <- c(1, 2, 3, 4, 5, 6)
##manipulate(hist(sample(outcomes, n, replace = TRUE), breaks = c(0.5, 1.5, 2.5,
    #3.5, 4.5, 5.5, 6.5), probability = TRUE, main = paste("Histogram of Outcomes of ",
    #n, " Die Rolls", sep = ""), xlab = "roll", ylab = "probability"), n = slider(0,
    #10000, initial = 10, step = 10))
```

**Rules of Probability**
*Positive probability*: Probability that something occurs
*Null probability*: Probability that nothing occurs
*Union symbol (U)*: essentially OR
*Intersection symbol (Upside down union)*: essentially AND
*Given symbol*: |
*Mutually exclusive*: cannot co-occur, probability is 0
*Bar above symbol*: means the complement of, essentially NOT
a. example - Pr(A with line above) is the same as Pr(not A)
*Independence*: the instance of one event does not affect the probability of the instance of another event

There are a bunch of equations for probabilities 

**Random Variables**
A variable whose outcomes are assumed to arise by chance or according to some random mechanism.

Two types
*Discrete*: countable
*Continuous*: can be measured to infinite values (ie. they can be any real number within a range). NOT countable. 

**Probability function**
A mathematical function that describes the chance associate with a random variable having a particular outcome or falling within a given range of outcomes. 

2 types:

1.*Probability mass functions (PMF)* are associated with discrete random variables. These functions describe the probability that a random variable takes a particular value.
To be a valid PMF...
  a. there are a set number of possible, distinct outcomes
  b. the probability of any outcome is greater than 0. 
  c. The sum of the probabilities of all outcomes is 1.
  
  *Cumulative Probability Graph*
    Used for Discrete Random Variables.Adds the probabilities of the outcomes. After all possible outcomes are added, the probability          should   be 1. 

2. *Probability density functions (PDF)* are associated with continuous random variables. These functions describe the probability that a random variable falls within a given range of outcomes. The area under the curve = the probability.
To be a valid PDF...
  a. the probability that the random variable is greater than a certain number is never negative ???
  b. the total area under the curve is 1.

The *Beta Distribution* refers to a family of continuous probability distributions defined over the interval [0, 1] and parametrized by two positive shape parameters, denoted by ð›¼and ð›½, that appear as exponents of the random variable ð‘¥ and control the shape of the distribution. (I don't fully understand what alpha and beta represent)

The *Cumulative Distribution Function (CDF)* of a random variable is defined as the probability of observing a random variable X taking the value of x or less. 
  
Like the Cumulative Probability Graph, we see a continuous curve increasing probability until we hit 1. This is essentially the continuous equivalent of the Cumulative Probability Graph.
  
**Beta functions** (*Can be used for non-beta distributions too, substitute the name of the distribution where beta is*)
pbeta(): input an outcome, returns the probability of the random variable assuming x or less. 
rbeta(): draws random simulated data from the distribution
dbeta(): returns the probability that an outcome takes on an exact inputted value 
qbeta(): input a probability/proportion, returns the exact outcome that has that proportion of the distribution at or below it

**Qth quantile**
The Qth quantile tells you the value at which Q% of data are at or below. 

**Expected Mean and Variance of Random Variables**
mu = population/expected mean
sigma squared = population/expected variance

**The Bernoulli Distribution**
This is essentially a single trial of a binomial distribution.

The probability of a binary random variable (only 2 possible outcomes).
Probability mass function for bernoulli distribution: P(X successes) = p^x(1-p)^(1-x)
Here, p represents the expected probability of success. 
Mu = p, and sigma squared = p(1-p)

*The Bernoulli distribution is a special case of the binomial distribution*
The bernoulli distribution is just one trial. The binomial distribution is essentially many bernoulli trials. 

Probability mass function: (n choose x)(p^x) * ((1-p)^(n-x))
Again, p represents the expected probability of success.
Mu = np
Sigma squared = np(1-p)

dbinom is the density function for the binomial distribution. dbinom(x = k, size = n, prob = p) where k is the number of successes, n is the number of trials, p is the probability of success.

**Poisson distribution**
Used to model open-ended counts. That is, the range of possible s Lower limit is 0, upper limit is infinity. 

**Q-Q Plots**
A Q-Q plot, or quantile-quantile plot, is a way to check if data are normally distributed. 
On the x axis: the theoretical quantiles pulled from a normal distribution
On the y axis: the actual quantiles of your data
*If the resulting plot is follows the line y=x, the data are normally distributed. 

**Sample Distributions vs. Population Distributions**
The above distributions and functions are all related to populations, but we rarely ever have data on a whole population.

We can basically use all of the above tests using sample means, sample variances. 

